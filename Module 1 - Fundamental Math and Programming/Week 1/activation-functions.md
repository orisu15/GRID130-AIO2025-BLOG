DÆ°á»›i Ä‘Ã¢y lÃ  phiÃªn báº£n cá»§a tÃ i liá»‡u Ä‘Ã£ Ä‘Æ°á»£c thÃªm emoji Ä‘á»ƒ tÄƒng pháº§n sinh Ä‘á»™ng vÃ  dá»… hiá»ƒu hÆ¡n:

---

# âš¡ Activation Functions

## ğŸ§® HÃ m Sigmoid

### ğŸ“˜ KhÃ¡i niá»‡m

HÃ m Sigmoid, cÃ²n Ä‘Æ°á»£c gá»i lÃ  hÃ m logistic, lÃ  má»™t trong nhá»¯ng hÃ m kÃ­ch hoáº¡t cÆ¡ báº£n nháº¥t trong há»c mÃ¡y vÃ  máº¡ng nÆ¡-ron. NÃ³ biáº¿n Ä‘á»•i báº¥t ká»³ giÃ¡ trá»‹ Ä‘áº§u vÃ o nÃ o thÃ nh giÃ¡ trá»‹ Ä‘áº§u ra náº±m trong khoáº£ng \$(0,1)\$.

HÃ m Sigmoid:

$\text{sigmoid}(x) = \frac{1}{1+e^{-x}}$

![ğŸ“ˆ Äá»“ thá»‹ Sigmoid](imgs/sigmoid-graph.png)

### ğŸ› ï¸ á»¨ng dá»¥ng

* âœ… PhÃ¢n loáº¡i nhá»‹ phÃ¢n

### ğŸ‘ Æ¯u Ä‘iá»ƒm

* ğŸ§  **Dá»… hiá»ƒu vÃ  dá»… triá»ƒn khai**: do tÃ­nh Ä‘Æ¡n giáº£n vÃ  phá»• biáº¿n, hÃ m sigmoid Ä‘Æ°á»£c cÃ i Ä‘áº·t trong nhiá»u loáº¡i máº¡ng nÆ¡-ron.
* ğŸ¯ **Pháº¡m vi Ä‘áº§u ra trong $\[0,1]\$**: dá»… dÃ ng diá»…n giáº£i nhÆ° xÃ¡c suáº¥t.

### ğŸ‘ NhÆ°á»£c Ä‘iá»ƒm

* âš ï¸ **Váº¥n Ä‘á» gradient biáº¿n máº¥t**: khi giÃ¡ trá»‹ Ä‘áº§u vÃ o quÃ¡ nhá» hoáº·c quÃ¡ lá»›n, Ä‘áº¡o hÃ m cá»§a sigmoid tiáº¿n Ä‘áº¿n 0 â†’ lÃ m cháº­m quÃ¡ trÃ¬nh há»c cá»§a máº¡ng.
* ğŸ”„ **KhÃ´ng Ä‘á»‘i xá»©ng quanh 0**: gÃ¢y khÃ³ khÄƒn trong Ä‘iá»u chá»‰nh trá»ng sá»‘ khi huáº¥n luyá»‡n.

### ğŸ’» Code

**ğŸ§¾ Code cÃ i Ä‘áº·t sigmoid function:**

```python
import math

def calc_sig(x):
    """
    TÃ­nh hÃ m sigmoid: Ïƒ(x) = 1 / (1 + e^(-x))
    """
    return 1./(1+math.e**(-x))

assert round(calc_sig(3), 2)==0.95

print(calc_sig(1))
```

**ğŸ“¤ Káº¿t quáº£:**

```
0.7310585786300049
```

---

## ğŸ”º HÃ m ReLU

### ğŸ“˜ KhÃ¡i niá»‡m

ReLU, viáº¿t táº¯t cá»§a â€œRectified Linear Unitâ€, lÃ  má»™t hÃ m kÃ­ch hoáº¡t ráº¥t phá»• biáº¿n trong máº¡ng nÆ¡-ron. ÄÆ¡n giáº£n nhÆ°ng hiá»‡u quáº£, ReLU khÃ´ng gáº·p pháº£i váº¥n Ä‘á» gradient biáº¿n máº¥t nhÆ° sigmoid.

HÃ m ReLU:

$$
\mathrm{relu}(x) = 
\begin{cases}
0 & \text{náº¿u } x \leq 0, \\
x & \text{náº¿u } x \gt 0.
\end{cases}
$$

![ğŸ“ˆ Äá»“ thá»‹ ReLU](imgs/relu-graph.png)

### ğŸ› ï¸ á»¨ng dá»¥ng

* ğŸ–¼ï¸ Nháº­n dáº¡ng áº£nh, ğŸ§¾ xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn: cáº£i thiá»‡n tá»‘c Ä‘á»™ há»c
* ğŸ•¹ï¸ Há»c tÄƒng cÆ°á»ng, bÃ i toÃ¡n phÃ¢n loáº¡i

### ğŸ‘ Æ¯u Ä‘iá»ƒm

* âš¡ **TÃ­nh toÃ¡n Ä‘Æ¡n giáº£n**: nhanh vÃ  hiá»‡u quáº£
* ğŸ’ª **Giáº£m gradient biáº¿n máº¥t**: giÃºp máº¡ng há»c tá»‘t hÆ¡n

### ğŸ‘ NhÆ°á»£c Ä‘iá»ƒm

* ğŸ’€ **Váº¥n Ä‘á» ReLU cháº¿t**: nÆ¡-ron ngá»«ng hoáº¡t Ä‘á»™ng náº¿u luÃ´n cho Ä‘áº§u ra 0
* ğŸ”„ **KhÃ´ng Ä‘á»‘i xá»©ng quanh 0**: áº£nh hÆ°á»Ÿng quÃ¡ trÃ¬nh tá»‘i Æ°u hÃ³a

### ğŸ’» Code

**ğŸ§¾ Code cÃ i Ä‘áº·t ReLU function:**

```python
def calc_relu(x):
    """
    TÃ­nh hÃ m ReLU:
    ReLU(x) = max(0, x)
    """
    if x<=0:
        result = 0.0
    else:
        result = x
    return float(result)

print(calc_relu(5))
```

**ğŸ“¤ Káº¿t quáº£:**

```
5.0
```

---

## ğŸ“‰ HÃ m ELU

### ğŸ“˜ KhÃ¡i niá»‡m

ELU, viáº¿t táº¯t cá»§a *Exponential Linear Unit*, lÃ  má»™t hÃ m kÃ­ch hoáº¡t hiá»‡n Ä‘áº¡i hÆ¡n, giÃºp kháº¯c phá»¥c Ä‘iá»ƒm yáº¿u cá»§a ReLU vÃ  sigmoid â€“ Ä‘áº·c biá»‡t lÃ  vá»›i giÃ¡ trá»‹ Ä‘áº§u vÃ o Ã¢m.

$$
\mathrm{ELU}(x) =
\begin{cases}
\alpha\bigl(e^x - 1\bigr) & \text{náº¿u } x \leq 0,\\
x & \text{náº¿u } x \gt 0.
\end{cases}
$$

![ğŸ“ˆ Äá»“ thá»‹ ELU](imgs/elu-graph.png)

### ğŸ› ï¸ á»¨ng dá»¥ng

* ğŸ§  **Máº¡ng nÆ¡-ron sÃ¢u**: giáº£m gradient biáº¿n máº¥t
* ğŸ—ï¸ **MÃ´ hÃ¬nh há»c sÃ¢u phá»©c táº¡p**: CNN, RNN â†’ tÄƒng hiá»‡u nÄƒng

### ğŸ‘ Æ¯u Ä‘iá»ƒm

* ğŸš€ **Hiá»‡u nÄƒng cao**: vÆ°á»£t trá»™i trong kiáº¿n trÃºc sÃ¢u
* ğŸ”„ **Cho ra giÃ¡ trá»‹ Ã¢m**: giÃºp cÃ¢n báº±ng Ä‘áº§u ra tá»‘t hÆ¡n

### ğŸ‘ NhÆ°á»£c Ä‘iá»ƒm

* ğŸ§® **TÃ­nh toÃ¡n phá»©c táº¡p hÆ¡n**: tá»‘n tÃ i nguyÃªn hÆ¡n so vá»›i ReLU
* ğŸ§ª **Lá»±a chá»n há»‡ sá»‘ \$\alpha\$**: cáº§n thá»­ nghiá»‡m Ä‘á»ƒ tÃ¬m giÃ¡ trá»‹ tá»‘i Æ°u

### ğŸ’» Code

**ğŸ§¾ Code cÃ i Ä‘áº·t ELU function:**

```python
import math

def calc_elu(x):
    """
    TÃ­nh hÃ m ELU (Exponential Linear Unit):
    ELU(x) = x                 náº¿u x >= 0
           = alpha * (e^x - 1) náº¿u x < 0
    """
    alpha = 0.01
    result = None
    if x < 0:
        result = alpha*(math.e**x - 1)
    else:
        result = x
    return result
assert round(calc_elu(1))==1

print(calc_elu(-4))
```

**ğŸ“¤ Káº¿t quáº£:**

```
-0.009816843611112657
```

---
